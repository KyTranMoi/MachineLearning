{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 1: Cài đặt giải thuật Apriori và ứng dụng giải thuật này cho 1 bộ dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(data):\n",
    "    \"\"\" Hàm này tạo các mục phổ biến kích thước 1 từ tập dữ liệu.\"\"\"\n",
    "    candidates = set()\n",
    "    for transaction in data:\n",
    "        for item in transaction:\n",
    "            candidates.add(frozenset([item]))\n",
    "    return candidates\n",
    "\n",
    "def prune_candidates(candidates, data, min_support):\n",
    "    \"\"\"Loại bỏ các mục không phổ biến từ tập ứng viên dựa trên mức hỗ trợ tối thiểu.\"\"\"\n",
    "    candidate_counts = {}\n",
    "    for transaction in data:\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction):\n",
    "                candidate_counts[candidate] = candidate_counts.get(candidate, 0) + 1\n",
    "\n",
    "    num_transactions = len(data)\n",
    "    frequent_candidates = []\n",
    "    support_data = {}\n",
    "    for candidate, count in candidate_counts.items():\n",
    "        support = count / num_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_candidates.append(candidate)\n",
    "        support_data[candidate] = support\n",
    "\n",
    "    return frequent_candidates, support_data\n",
    "\n",
    "def create_new_candidates(old_candidates, k):\n",
    "    \"\"\" Tạo các tập ứng viên mới từ các tập phổ biến cũ\"\"\"\n",
    "    new_candidates = []\n",
    "    num_old_candidates = len(old_candidates)\n",
    "    for i in range(num_old_candidates):\n",
    "        for j in range(i + 1, num_old_candidates):\n",
    "            itemset1 = list(old_candidates[i])[:k - 2]\n",
    "            itemset2 = list(old_candidates[j])[:k - 2]\n",
    "            itemset1.sort()\n",
    "            itemset2.sort()\n",
    "            if itemset1 == itemset2:\n",
    "                new_candidates.append(old_candidates[i] | old_candidates[j])\n",
    "    return new_candidates\n",
    "\n",
    "def apriori(data, min_support):\n",
    "    candidates = create_candidates(data)\n",
    "    frequent_items, support_data = prune_candidates(candidates, data, min_support)\n",
    "    all_frequent_items = [frequent_items]\n",
    "    k = 2\n",
    "    while len(all_frequent_items[-1]) > 0:\n",
    "        new_candidates = create_new_candidates(all_frequent_items[-1], k)\n",
    "        frequent_items, new_support_data = prune_candidates(new_candidates, data, min_support)\n",
    "        support_data.update(new_support_data)\n",
    "        all_frequent_items.append(frequent_items)\n",
    "        k += 1\n",
    "    return all_frequent_items, support_data\n",
    "\n",
    "def generate_rules(frequent_itemsets, support_data, min_confidence):\n",
    "    \"\"\" Tạo các luật từ các tập phổ biến và dữ liệu hỗ trợ.\"\"\"\n",
    "    rules = []\n",
    "    for k_itemset in frequent_itemsets[1:]:\n",
    "        for itemset in k_itemset:\n",
    "            for item in itemset:\n",
    "                antecedent = itemset - set([item])\n",
    "                consequent = set([item])\n",
    "                confidence = support_data[itemset] / support_data[antecedent]\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((antecedent, consequent, confidence))\n",
    "    return rules\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['B', 'C', 'D'],\n",
    "        ['A', 'C'],\n",
    "        ['A', 'D'],\n",
    "        ['B', 'D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các tập mặt hàng phổ biến:\n",
      "Kích thước 1: [frozenset({'B'}), frozenset({'D'}), frozenset({'C'}), frozenset({'A'})]\n",
      "Kích thước 2: [frozenset({'B', 'D'}), frozenset({'B', 'C'}), frozenset({'D', 'C'}), frozenset({'A', 'C'}), frozenset({'D', 'A'})]\n",
      "Kích thước 3: [frozenset({'B', 'D', 'C'})]\n",
      "Kích thước 4: []\n",
      "\n",
      "Dữ liệu hỗ trợ:\n",
      "frozenset({'B'}): 0.5\n",
      "frozenset({'D'}): 0.75\n",
      "frozenset({'C'}): 0.5\n",
      "frozenset({'A'}): 0.5\n",
      "frozenset({'B', 'D'}): 0.5\n",
      "frozenset({'B', 'C'}): 0.25\n",
      "frozenset({'D', 'C'}): 0.25\n",
      "frozenset({'A', 'C'}): 0.25\n",
      "frozenset({'D', 'A'}): 0.25\n",
      "frozenset({'B', 'D', 'C'}): 0.25\n",
      "Các luật kết hợp:\n",
      "['B'] => ['D'] (Confidence: 1.0)\n",
      "['D', 'C'] => ['B'] (Confidence: 1.0)\n",
      "['B', 'C'] => ['D'] (Confidence: 1.0)\n"
     ]
    }
   ],
   "source": [
    "min_support = 0.2\n",
    "\n",
    "frequent_itemsets, support_data = apriori(data, min_support)\n",
    "\n",
    "print(\"Các tập mặt hàng phổ biến:\")\n",
    "for k, itemsets in enumerate(frequent_itemsets):\n",
    "    print(f\"Kích thước {k+1}: {itemsets}\")\n",
    "\n",
    "print(\"\\nDữ liệu hỗ trợ:\")\n",
    "for itemset, support in support_data.items():\n",
    "    print(f\"{itemset}: {support}\")\n",
    "    \n",
    "min_confidence = 0.8\n",
    "rules = generate_rules(frequent_itemsets, support_data, min_confidence)\n",
    "\n",
    "print(\"Các luật kết hợp:\")\n",
    "for antecedent, consequent, confidence in rules:\n",
    "    print(f\"{list(antecedent)} => {list(consequent)} (Confidence: {confidence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kiểm tra lại với hàm thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support   itemsets\n",
      "0     0.50        (A)\n",
      "1     0.50        (B)\n",
      "2     0.50        (C)\n",
      "3     0.75        (D)\n",
      "4     0.25     (C, A)\n",
      "5     0.25     (D, A)\n",
      "6     0.25     (B, C)\n",
      "7     0.50     (B, D)\n",
      "8     0.25     (D, C)\n",
      "9     0.25  (B, D, C)\n",
      "frozenset({'B'}) => frozenset({'D'}) (Confidence: 1.0)\n",
      "frozenset({'B', 'C'}) => frozenset({'D'}) (Confidence: 1.0)\n",
      "frozenset({'D', 'C'}) => frozenset({'B'}) (Confidence: 1.0)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "encoder = TransactionEncoder()\n",
    "transactions_encoded = encoder.fit(data).transform(data)\n",
    "df = pd.DataFrame(transactions_encoded, columns=encoder.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "for i in rules.index:\n",
    "    if rules.loc[i, \"confidence\"]>=min_confidence:\n",
    "        print(rules.loc[i, 'antecedents'], '=>', rules.loc[i, 'consequents'], f'(Confidence: {rules.loc[i, \"confidence\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 2:  Cài đặt giải thuật FP-Growth và ứng dụng giải thuật này cho 1 bộ dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ hỗ trợ của các tập mặt hàng phổ biến:\n",
      "{'B'}: 0.5\n",
      "{'C'}: 0.5\n",
      "{'A'}: 0.5\n",
      "{'D'}: 0.75\n",
      "{'B', 'D'}: 0.5\n",
      "Các luật kết hợp:\n",
      "frozenset({'B'}) => {'D'} (Confidence: 1.0)\n"
     ]
    }
   ],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, item, frequency, parent):\n",
    "        self.item = item\n",
    "        self.frequency = frequency\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.next_link = None\n",
    "\n",
    "def construct_tree(dataset, min_support):\n",
    "    \"\"\" hàm này xây dựng cây FP-Growth từ tập dữ liệu với min_support\"\"\"\n",
    "    header_table = {}\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            header_table[item] = header_table.get(item, 0) + dataset[transaction]\n",
    "\n",
    "    for item in list(header_table.keys()):\n",
    "        if header_table[item] < min_support:\n",
    "            del(header_table[item])\n",
    "\n",
    "    frequent_items = set(header_table.keys())\n",
    "\n",
    "    if len(frequent_items) == 0:\n",
    "        return None, None\n",
    "\n",
    "    for item in header_table:\n",
    "        header_table[item] = [header_table[item], None]\n",
    "\n",
    "    fp_tree = TreeNode(\"Null\", 1, None)\n",
    "    for transaction, frequency in dataset.items():\n",
    "        transaction_sorted = []\n",
    "        for item in transaction:\n",
    "            if item in frequent_items:\n",
    "                transaction_sorted.append(item)\n",
    "        if len(transaction_sorted) > 0:\n",
    "            update_tree(fp_tree, transaction_sorted, header_table, frequency)\n",
    "\n",
    "    return fp_tree, header_table\n",
    "\n",
    "def update_tree(current_node, transaction_sorted, header_table, frequency):\n",
    "    \"\"\"hàm này cập nhật cây FP-Growth với một giao dịch đã được sắp xếp và một bảng tiêu đề.\"\"\"\n",
    "    if transaction_sorted[0] in current_node.children:\n",
    "        current_node.children[transaction_sorted[0]].frequency += frequency\n",
    "    else:\n",
    "        current_node.children[transaction_sorted[0]] = TreeNode(transaction_sorted[0], frequency, current_node)\n",
    "\n",
    "        if header_table[transaction_sorted[0]][1] is None:\n",
    "            header_table[transaction_sorted[0]][1] = current_node.children[transaction_sorted[0]]\n",
    "        else:\n",
    "            update_header(header_table[transaction_sorted[0]][1], current_node.children[transaction_sorted[0]])\n",
    "\n",
    "    if len(transaction_sorted) > 1:\n",
    "        update_tree(current_node.children[transaction_sorted[0]], transaction_sorted[1:], header_table, frequency)\n",
    "\n",
    "def update_header(node_to_test, target_node):\n",
    "    \"\"\"hàm này cập nhật liên kết tiếp theo của một nút trong cây FP-Growth.\"\"\"\n",
    "    while node_to_test.next_link is not None:\n",
    "        node_to_test = node_to_test.next_link\n",
    "    node_to_test.next_link = target_node\n",
    "\n",
    "def ascend_tree(node, prefix_path):\n",
    "    \"\"\" hàm này tạo một đường dẫn từ một nút trong cây FP-Growth đến gốc.\"\"\"\n",
    "    if node.parent is not None:\n",
    "        prefix_path.append(node.item)\n",
    "        ascend_tree(node.parent, prefix_path)\n",
    "\n",
    "def find_prefix_path(base_item, header_table):\n",
    "    \"\"\"Hàm này tìm các đường dẫn tiền tố của một mục trong cây FP-Growth và bảng tiêu đề.\"\"\"\n",
    "    tree_node = header_table[base_item][1]\n",
    "    prefix_paths = {}\n",
    "    while tree_node is not None:\n",
    "        prefix_path = []\n",
    "        ascend_tree(tree_node, prefix_path)\n",
    "        if len(prefix_path) > 1:\n",
    "            prefix_paths[frozenset(prefix_path[1:])] = tree_node.frequency\n",
    "        tree_node = tree_node.next_link\n",
    "    return prefix_paths\n",
    "\n",
    "def mine_patterns(header_table, min_support, prefix, frequent_itemsets):\n",
    "    \"\"\" hàm này tìm các tập phổ biến từ cây FP-Growth và bảng tiêu đề.\"\"\"\n",
    "    bigL = [v[0] for v in sorted(header_table.items(), key=lambda p: p[1][0])]\n",
    "\n",
    "    for base_item in bigL:\n",
    "        new_frequent_set = prefix.copy()\n",
    "        new_frequent_set.add(base_item)\n",
    "        frequent_itemsets.append(new_frequent_set)\n",
    "        conditional_tree_paths = find_prefix_path(base_item, header_table)\n",
    "        conditional_tree, conditional_header = construct_tree(conditional_tree_paths, min_support)\n",
    "        if conditional_header is not None:\n",
    "            mine_patterns(conditional_header, min_support, new_frequent_set, frequent_itemsets)\n",
    "\n",
    "def fpgrowth(dataset, min_support):\n",
    "    fp_tree, header_table = construct_tree(dataset, min_support)\n",
    "    frequent_itemsets = []\n",
    "    mine_patterns(header_table, min_support, set([]), frequent_itemsets)\n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            rules.extend(generate_rules_from_itemset(itemset, min_confidence))\n",
    "    return rules\n",
    "\n",
    "def generate_rules_from_itemset(itemset, min_confidence):\n",
    "    rules = []\n",
    "    for item in itemset:\n",
    "        antecedent = frozenset([item])\n",
    "        consequent = itemset - antecedent\n",
    "        confidence = support_data[frozenset(itemset)] / support_data[antecedent]\n",
    "        if confidence >= min_confidence:\n",
    "            rules.append((antecedent, consequent, confidence))\n",
    "    return rules\n",
    "\n",
    "\n",
    "transaction_counts = {}\n",
    "for transaction in data:\n",
    "    transaction_counts[frozenset(transaction)] = transaction_counts.get(frozenset(transaction), 0) + 1\n",
    "\n",
    "min_support = 2 \n",
    "frequent_itemsets = fpgrowth(transaction_counts, min_support)\n",
    "\n",
    "print(\"Độ hỗ trợ của các tập mặt hàng phổ biến:\")\n",
    "for itemset in frequent_itemsets:\n",
    "    support = support_data[frozenset(itemset)]\n",
    "    if support >= min_support/len(data):\n",
    "        print(f\"{itemset}: {support}\")\n",
    "\n",
    "min_confidence = 0.7\n",
    "rules = generate_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "print(\"Các luật kết hợp:\")\n",
    "for antecedent, consequent, confidence in rules:\n",
    "    print(f\"{antecedent} => {consequent} (Confidence: {confidence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kiểm tra lại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support itemsets\n",
      "0     0.75      (D)\n",
      "1     0.50      (C)\n",
      "2     0.50      (B)\n",
      "3     0.50      (A)\n",
      "4     0.50   (B, D)\n",
      "frozenset({'B'}) => frozenset({'D'}) (Confidence: 1.0)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "\n",
    "encoder = TransactionEncoder()\n",
    "transactions_encoded = encoder.fit(data).transform(data)\n",
    "df = pd.DataFrame(transactions_encoded, columns=encoder.columns_)\n",
    "frequent_itemsets = fpgrowth(df, min_support=min_support/len(data), use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "for i in rules.index:\n",
    "    if rules.loc[i, \"confidence\"]>=min_confidence:\n",
    "        print(rules.loc[i, 'antecedents'], '=>', rules.loc[i, 'consequents'], f'(Confidence: {rules.loc[i, \"confidence\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 3:  Cài đặt giải thuật cải tiến sau: (Source: A Sliding Window Based Approach for Mining Frequent Weighted Patterns Over Data Streams) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWNNode:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weight = 0\n",
    "        self.children = {}\n",
    "        self.pre = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    def insert(self, transaction, tw):\n",
    "        if transaction:\n",
    "            first_item = transaction[0]\n",
    "            remaining_items = transaction[1:]\n",
    "            if first_item in self.children:\n",
    "                self.children[first_item].weight += tw\n",
    "            else:\n",
    "                self.children[first_item] = SWNNode(first_item)\n",
    "                self.children[first_item].weight = tw\n",
    "            self.children[first_item].insert(remaining_items, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWNTree:\n",
    "    def __init__(self):\n",
    "        self.root = SWNNode('null')\n",
    "\n",
    "    def insert_tree(self, transaction, tw):\n",
    "        self.root.insert(transaction, tw)\n",
    "\n",
    "    def generate_pre_pos(self, node=None, pre=0):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        node.pre = pre\n",
    "        for child in node.children.values():\n",
    "            pre += 1\n",
    "            pre = self.generate_pre_pos(child, pre)\n",
    "        node.pos = pre\n",
    "        return pre\n",
    "\n",
    "    def construct_swn_tree(self, db, tw):\n",
    "        for transaction in db:\n",
    "            transaction.sort(key=lambda item: -item[1])  # Sort by frequency\n",
    "            items = [item[0] for item in transaction]  # Keep only the item names\n",
    "            self.insert_tree(items, tw)\n",
    "        self.generate_pre_pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null (weight: 0, pre: 0, pos: 14)\n",
      "  A (weight: 1, pre: 1, pos: 3)\n",
      "    B (weight: 1, pre: 2, pos: 3)\n",
      "      E (weight: 1, pre: 3, pos: 3)\n",
      "  B (weight: 2, pre: 4, pos: 7)\n",
      "    c (weight: 1, pre: 5, pos: 5)\n",
      "    E (weight: 1, pre: 6, pos: 7)\n",
      "      c (weight: 1, pre: 7, pos: 7)\n",
      "  E (weight: 1, pre: 8, pos: 10)\n",
      "    A (weight: 1, pre: 9, pos: 10)\n",
      "      F (weight: 1, pre: 10, pos: 10)\n",
      "  F (weight: 1, pre: 11, pos: 14)\n",
      "    A (weight: 1, pre: 12, pos: 14)\n",
      "      B (weight: 1, pre: 13, pos: 14)\n",
      "        c (weight: 1, pre: 14, pos: 14)\n"
     ]
    }
   ],
   "source": [
    "db = [\n",
    "    [('A', 2), ('B', 1), ('E', 1)],\n",
    "    [('B', 3), ('c', 2)],\n",
    "    [('A', 1), ('E', 2), ('F', 1)],\n",
    "    [('B', 2), ('E', 1), ('c', 1)],\n",
    "    [('A', 1), ('B', 1), ('c', 1), ('F', 2)]\n",
    "]\n",
    "\n",
    "tw = 1\n",
    "swn_tree = SWNTree()\n",
    "swn_tree.construct_swn_tree(db, tw)\n",
    "\n",
    "# In ra cây SWN để kiểm tra\n",
    "def print_tree(node, indent=0):\n",
    "    print('  ' * indent + f'{node.name} (weight: {node.weight}, pre: {node.pre}, pos: {node.pos})')\n",
    "    for child in node.children.values():\n",
    "        print_tree(child, indent + 1)\n",
    "\n",
    "print_tree(swn_tree.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
